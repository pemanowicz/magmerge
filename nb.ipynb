{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb03876",
   "metadata": {},
   "source": [
    "# Load & execute pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dccd119",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_stage_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# USE EXAMPLE\u001b[39;00m\n\u001b[32m     96\u001b[39m df_bin = pipeline_Binning(\u001b[33m\"\u001b[39m\u001b[33mpython_paths.csv\u001b[39m\u001b[33m\"\u001b[39m, print_paths =\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m df_cov = \u001b[43mpipeline_COVERAGE\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpython_paths.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_paths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m df_gtdb = pipeline_GTDBTK(\u001b[33m\"\u001b[39m\u001b[33mpython_paths.csv\u001b[39m\u001b[33m\"\u001b[39m, print_paths =\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mpipeline_COVERAGE\u001b[39m\u001b[34m(paths_csv, print_paths)\u001b[39m\n\u001b[32m     77\u001b[39m     df.columns = [col.lstrip(\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns]\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_stage_files\u001b[49m(paths_csv, \u001b[33m\"\u001b[39m\u001b[33mCOVERAGE\u001b[39m\u001b[33m\"\u001b[39m, build_paths, reader, print_paths)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_stage_files' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "#from src.magmerge.load_paths import load_stage_files\n",
    "\n",
    "# PIPELINE: BINNING\n",
    "def pipeline_Binning(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        return [\n",
    "            folder / f\"{sample_id}_DASTool_contig2bin.tsv\",\n",
    "            folder / f\"{sample_id}_DASTool_summary.tsv\"\n",
    "        ]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        if path.name.endswith(\"contig2bin.tsv\"):\n",
    "            return pd.read_csv(\n",
    "                path, sep=\"\\t\", header=None,\n",
    "                names=[\"contig\", \"bin\"],\n",
    "                dtype={\"contig\": \"string\", \"bin\": \"string\"},\n",
    "            )\n",
    "        elif path.name.endswith(\"summary.tsv\"):\n",
    "            return pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # special contig2bin join with summary → outer join after bin\n",
    "    df_paths = pd.read_csv(paths_csv, sep=\",\", dtype=str,)\n",
    "    bin_rows = df_paths[df_paths[\"stage\"] == \"BINNING\"].copy()\n",
    "    frames: list[pd.DataFrame] = []\n",
    "\n",
    "    for _, row in bin_rows.iterrows():\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        contig2bin_path = folder / f\"{sample_id}_DASTool_contig2bin.tsv\"\n",
    "        summary_path   = folder / f\"{sample_id}_DASTool_summary.tsv\"\n",
    "\n",
    "        if print_paths:\n",
    "            logger.info(contig2bin_path)\n",
    "            logger.info(summary_path)\n",
    "\n",
    "        try:\n",
    "            c2b = pd.read_csv(contig2bin_path, sep=\"\\t\", header=None,\n",
    "                              names=[\"contig\", \"bin\"], dtype=\"string\")\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File not found: {contig2bin_path}\")\n",
    "            c2b = pd.DataFrame(columns=[\"contig\", \"bin\"])\n",
    "\n",
    "        try:\n",
    "            summ = pd.read_csv(summary_path, sep=\"\\t\", dtype=\"string\")\n",
    "            if \"bin\" not in summ.columns:\n",
    "                summ = pd.DataFrame(columns=[\"bin\"])\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File not found: {summary_path}\")\n",
    "            summ = pd.DataFrame(columns=[\"bin\"])\n",
    "\n",
    "        merged = c2b.merge(summ, on=\"bin\", how=\"outer\")\n",
    "        frames.append(merged)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"contig\", \"bin\"])\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# PIPELINE: COVERAGE\n",
    "def pipeline_COVERAGE(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        return [folder / f\"{sample_id}_coverage.tsv\"]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "        df.columns = [col.lstrip(\"#\") for col in df.columns]\n",
    "        return df\n",
    "\n",
    "    return load_stage_files(paths_csv, \"COVERAGE\", build_paths, reader, print_paths)\n",
    "\n",
    "\n",
    "# PIPELINE: GTDBTK\n",
    "def pipeline_GTDBTK(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        return [folder / \"gtdbtk.bac120.summary.tsv\"]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        return pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "\n",
    "    return load_stage_files(paths_csv, \"GTDBTK\", build_paths, reader, print_paths)\n",
    "\n",
    "\n",
    "# USE EXAMPLE\n",
    "df_bin = pipeline_Binning(\"python_paths.csv\", print_paths =False)\n",
    "df_cov = pipeline_COVERAGE(\"python_paths.csv\", print_paths =False)\n",
    "df_gtdb = pipeline_GTDBTK(\"python_paths.csv\", print_paths =False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01f03",
   "metadata": {},
   "source": [
    "# Merge & save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e950c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-03 11:22:57.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_mag_table\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mUsunięto 14 z 48 rekordów z brakami (NaN/NULL).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 mag_id  genome_size  bin_score  relative_abundance    Domain  \\\n",
      "0  ERR321064_concoct_12      2931697   0.960784            0.011487  Bacteria   \n",
      "1   ERR321064_concoct_8      3829513   0.784314             0.01397  Bacteria   \n",
      "2   ERR321064_concoct_9      2619783   0.859216            0.015695  Bacteria   \n",
      "3  ERR321097_SemiBin_16      1271595   0.578684            0.008242  Bacteria   \n",
      "4  ERR321097_SemiBin_17      1929185   0.980392            0.007278  Bacteria   \n",
      "\n",
      "         Phylum        Class            Order           Family  \\\n",
      "0  Bacteroidota  Bacteroidia    Bacteroidales    Rikenellaceae   \n",
      "1  Bacteroidota  Bacteroidia    Bacteroidales   Bacteroidaceae   \n",
      "2   Bacillota_A   Clostridia   Lachnospirales  Lachnospiraceae   \n",
      "3   Bacillota_A   Clostridia  Oscillospirales  Ruminococcaceae   \n",
      "4   Bacillota_A   Clostridia   Lachnospirales  Lachnospiraceae   \n",
      "\n",
      "            Genus                     Species closest_reference_genome_id  \\\n",
      "0       Alistipes       Alistipes onderdonkii             GCF_025145285.1   \n",
      "1     Bacteroides       Bacteroides uniformis             GCF_025147485.1   \n",
      "2    Agathobacter       Agathobacter rectalis             GCF_000020605.1   \n",
      "3        Gemmiger         Gemmiger qucibialis             GCA_004000625.1   \n",
      "4  Butyrivibrio_A  Butyrivibrio_A sp000431815             GCA_000431815.1   \n",
      "\n",
      "   closest_reference_genome_ani  \n",
      "0                         96.87  \n",
      "1                         98.18  \n",
      "2                         98.06  \n",
      "3                         98.72  \n",
      "4                         99.15  \n"
     ]
    }
   ],
   "source": [
    "def _split_taxonomy(classif: str) -> dict:\n",
    "    cols = {\"Domain\": None, \"Phylum\": None, \"Class\": None,\n",
    "            \"Order\": None, \"Family\": None, \"Genus\": None, \"Species\": None}\n",
    "    if not isinstance(classif, str):\n",
    "        return cols\n",
    "    for token in classif.split(\";\"):\n",
    "        if \"__\" not in token:\n",
    "            continue\n",
    "        prefix, name = token.split(\"__\", 1)\n",
    "        if   prefix == \"d\": cols[\"Domain\"]  = name\n",
    "        elif prefix == \"p\": cols[\"Phylum\"]  = name\n",
    "        elif prefix == \"c\": cols[\"Class\"]   = name\n",
    "        elif prefix == \"o\": cols[\"Order\"]   = name\n",
    "        elif prefix == \"f\": cols[\"Family\"]  = name\n",
    "        elif prefix == \"g\": cols[\"Genus\"]   = name\n",
    "        elif prefix == \"s\": cols[\"Species\"] = name\n",
    "    return cols\n",
    "\n",
    "def prepare_mag_table(df_gtdb: pd.DataFrame, df_cov: pd.DataFrame, df_bin: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "Builds the final MAG table as required.\n",
    "Expected inputs:\n",
    "- df_gtdb: columns at least ['user_genome','classification','closest_genome_reference','closest_genome_ani']\n",
    "- df_cov: columns at least ['rname','endpos','numreads'] (+ optional 'sample_id')\n",
    "- df_bin: columns at least ['contig','bin'] + (from DASTool_summary.tsv) 'bin_score'\n",
    "Returns a DataFrame with columns:\n",
    "['mag_id','genome_size','bin_score','relative_abundance',\n",
    "'Domain','Phylum','Class','Order','Family','Genus','Species', 'closest_reference_genome_id','closest_reference_genome_ani']\n",
    "and prints how many records were rejected due to missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) map contig->bin and connect to coverage\n",
    "    # sanity dtype\n",
    "    cov = df_cov.copy()\n",
    "    cov.columns = [str(c).lstrip(\"#\") for c in cov.columns]\n",
    "    cov[\"endpos\"]   = pd.to_numeric(cov[\"endpos\"], errors=\"coerce\")\n",
    "    cov[\"numreads\"] = pd.to_numeric(cov[\"numreads\"], errors=\"coerce\")\n",
    "\n",
    "    contig2bin = df_bin[[\"contig\", \"bin\"]].dropna().copy()\n",
    "\n",
    "    cov_bin = cov.merge(contig2bin, left_on=\"rname\", right_on=\"contig\", how=\"inner\")\n",
    "\n",
    "# 2) genome_size: sum of contig lengths in the bin\n",
    "# I take the contig length as endpos (coverage counted from 1 to endpos)\n",
    "    contig_len = (cov_bin\n",
    "                  .groupby([\"bin\", \"rname\"], as_index=False)[\"endpos\"]\n",
    "                  .max())  # na wypadek duplikatów rname w pliku\n",
    "    genome_size = (contig_len\n",
    "                   .groupby(\"bin\", as_index=False)[\"endpos\"]\n",
    "                   .sum()\n",
    "                   .rename(columns={\"bin\": \"mag_id\", \"endpos\": \"genome_size\"}))\n",
    "\n",
    "    # 3) relative abundance: share of readings per bin\n",
    "    if \"sample_id\" in cov_bin.columns:\n",
    "        reads_per = (cov_bin.groupby([\"sample_id\", \"bin\"], as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = (cov_bin.groupby(\"sample_id\", as_index=False)[\"numreads\"]\n",
    "                       .sum()\n",
    "                       .rename(columns={\"numreads\": \"reads_total\"}))\n",
    "        rel = reads_per.merge(total_reads, on=\"sample_id\", how=\"left\")\n",
    "        rel[\"relative_abundance\"] = rel[\"reads_in_bin\"] / rel[\"reads_total\"]\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "# If I have multiple samples, duplicate mag_ids from different samples may result.\n",
    "# Consolidate by sum (or average). By default, I'll take the sum of the contributions (typically 1 sample => no influence).\n",
    "        rel = rel.groupby(\"mag_id\", as_index=False)[\"relative_abundance\"].sum()\n",
    "    else:\n",
    "        reads_per = (cov_bin.groupby(\"bin\", as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = reads_per[\"reads_in_bin\"].sum()\n",
    "        rel = reads_per.assign(relative_abundance=reads_per[\"reads_in_bin\"] / total_reads)\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "\n",
    "    # 4) bin_score from DASTool_summary\n",
    "    # Take unique bin_score per bin (sometimes repeated per contig).\n",
    "    if \"bin_score\" in df_bin.columns:\n",
    "        bs = (df_bin[[\"bin\", \"bin_score\"]]\n",
    "              .dropna(subset=[\"bin\"])\n",
    "              .drop_duplicates(subset=[\"bin\"]))\n",
    "        bs[\"bin_score\"] = pd.to_numeric(bs[\"bin_score\"], errors=\"coerce\")\n",
    "        bs = bs.rename(columns={\"bin\": \"mag_id\"})\n",
    "    else:\n",
    "        # if no column in input\n",
    "        bs = pd.DataFrame(columns=[\"mag_id\", \"bin_score\"])\n",
    "\n",
    "    # 5) GTDB: taxonomy + closest genome\n",
    "    gtdb = df_gtdb[[\"user_genome\", \"classification\",\n",
    "                    \"closest_genome_reference\", \"closest_genome_ani\"]].copy()\n",
    "\n",
    "    tax = gtdb[\"classification\"].apply(_split_taxonomy).apply(pd.Series)\n",
    "    gtdb_clean = pd.concat([gtdb.drop(columns=[\"classification\"]), tax], axis=1)\n",
    "    gtdb_clean = gtdb_clean.rename(columns={\n",
    "        \"user_genome\": \"mag_id\",\n",
    "        \"closest_genome_reference\": \"closest_reference_genome_id\",\n",
    "        \"closest_genome_ani\": \"closest_reference_genome_ani\"\n",
    "    })\n",
    "    gtdb_clean[\"closest_reference_genome_ani\"] = pd.to_numeric(\n",
    "        gtdb_clean[\"closest_reference_genome_ani\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 6) Merging everything by mag_id\n",
    "    merged = (genome_size\n",
    "              .merge(rel, on=\"mag_id\", how=\"left\")\n",
    "              .merge(bs, on=\"mag_id\",  how=\"left\")\n",
    "              .merge(gtdb_clean, on=\"mag_id\", how=\"left\"))\n",
    "\n",
    "    # 7) First select only the required columns\n",
    "    wanted = [\n",
    "        \"mag_id\",\n",
    "        \"genome_size\",\n",
    "        \"bin_score\",\n",
    "        \"relative_abundance\",\n",
    "        \"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\",\n",
    "        \"closest_reference_genome_id\",\n",
    "        \"closest_reference_genome_ani\",\n",
    "    ]\n",
    "    out = merged[wanted].copy()\n",
    "\n",
    "    # Now remove missing records and report\n",
    "    before = len(out)\n",
    "    out_clean = out.dropna()\n",
    "    removed = before - len(out_clean)\n",
    "    logger.info(f\"Usunięto {removed} z {before} rekordów z brakami (NaN/NULL).\")\n",
    "\n",
    "    return out_clean\n",
    "\n",
    "df_mag = prepare_mag_table(df_gtdb, df_cov, df_bin)\n",
    "\n",
    "# Preview of the first few lines\n",
    "print(df_mag.head())\n",
    "\n",
    "# Writing to a file\n",
    "df_mag.to_csv(\"MAG_table.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ed894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
