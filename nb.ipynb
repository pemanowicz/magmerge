{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb03876",
   "metadata": {},
   "source": [
    "# Load & execute pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def _load_stage_files(paths_csv: str, stage: str, build_paths_fn, reader_fn, print_paths: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Universal loader for files referenced in `python_paths.csv`.\n",
    "\n",
    ":param paths_csv: CSV file with columns: study_id, sample_id, stage, folder\n",
    ":param stage: name of the stage (BINNING, COVERAGE, GTDBTK)\n",
    ":param build_paths_fn: function (row: pd.Series) -> List[Path]\n",
    ":param reader_fn: function (Path) -> pd.DataFrame (may return an empty DataFrame if the file is missing)\n",
    ":param print_paths: whether to print the file paths\n",
    ":return: concatenated DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    df_paths = pd.read_csv(paths_csv, sep=\",\", dtype=str,)\n",
    "    rows = df_paths[df_paths[\"stage\"] == stage].copy()\n",
    "    frames: list[pd.DataFrame] = []\n",
    "\n",
    "    for _, row in rows.iterrows():\n",
    "        for path in build_paths_fn(row):\n",
    "            if print_paths:\n",
    "                print(path)\n",
    "            try:\n",
    "                df_part = reader_fn(path)\n",
    "                frames.append(df_part)\n",
    "            except FileNotFoundError:\n",
    "                logger.warning(f\"File not found: {path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading file {path}: {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# PIPELINE: BINNING\n",
    "def pipeline_Binning(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        return [\n",
    "            folder / f\"{sample_id}_DASTool_contig2bin.tsv\",\n",
    "            folder / f\"{sample_id}_DASTool_summary.tsv\"\n",
    "        ]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        if path.name.endswith(\"contig2bin.tsv\"):\n",
    "            return pd.read_csv(\n",
    "                path, sep=\"\\t\", header=None,\n",
    "                names=[\"contig\", \"bin\"],\n",
    "                dtype={\"contig\": \"string\", \"bin\": \"string\"},\n",
    "            )\n",
    "        elif path.name.endswith(\"summary.tsv\"):\n",
    "            return pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # special contig2bin join with summary → outer join after bin\n",
    "    df_paths = pd.read_csv(paths_csv, sep=\",\", dtype=str,)\n",
    "    bin_rows = df_paths[df_paths[\"stage\"] == \"BINNING\"].copy()\n",
    "    frames: list[pd.DataFrame] = []\n",
    "\n",
    "    for _, row in bin_rows.iterrows():\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        contig2bin_path = folder / f\"{sample_id}_DASTool_contig2bin.tsv\"\n",
    "        summary_path   = folder / f\"{sample_id}_DASTool_summary.tsv\"\n",
    "\n",
    "        if print_paths:\n",
    "            logger.info(contig2bin_path)\n",
    "            logger.info(summary_path)\n",
    "\n",
    "        try:\n",
    "            c2b = pd.read_csv(contig2bin_path, sep=\"\\t\", header=None,\n",
    "                              names=[\"contig\", \"bin\"], dtype=\"string\")\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File not found: {contig2bin_path}\")\n",
    "            c2b = pd.DataFrame(columns=[\"contig\", \"bin\"])\n",
    "\n",
    "        try:\n",
    "            summ = pd.read_csv(summary_path, sep=\"\\t\", dtype=\"string\")\n",
    "            if \"bin\" not in summ.columns:\n",
    "                summ = pd.DataFrame(columns=[\"bin\"])\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File not found: {summary_path}\")\n",
    "            summ = pd.DataFrame(columns=[\"bin\"])\n",
    "\n",
    "        merged = c2b.merge(summ, on=\"bin\", how=\"outer\")\n",
    "        frames.append(merged)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"contig\", \"bin\"])\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# PIPELINE: COVERAGE\n",
    "def pipeline_COVERAGE(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        sample_id = row[\"sample_id\"]\n",
    "        return [folder / f\"{sample_id}_coverage.tsv\"]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "        df.columns = [col.lstrip(\"#\") for col in df.columns]\n",
    "        return df\n",
    "\n",
    "    return _load_stage_files(paths_csv, \"COVERAGE\", build_paths, reader, print_paths)\n",
    "\n",
    "\n",
    "# PIPELINE: GTDBTK\n",
    "def pipeline_GTDBTK(paths_csv: str, print_paths: bool = True) -> pd.DataFrame:\n",
    "    def build_paths(row):\n",
    "        folder = Path(row[\"folder\"])\n",
    "        return [folder / \"gtdbtk.bac120.summary.tsv\"]\n",
    "\n",
    "    def reader(path: Path) -> pd.DataFrame:\n",
    "        return pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "\n",
    "    return _load_stage_files(paths_csv, \"GTDBTK\", build_paths, reader, print_paths)\n",
    "\n",
    "\n",
    "# USE EXAMPLE\n",
    "df_bin = pipeline_Binning(\"python_paths.csv\", print_paths =False)\n",
    "df_cov = pipeline_COVERAGE(\"python_paths.csv\", print_paths =False)\n",
    "df_gtdb = pipeline_GTDBTK(\"python_paths.csv\", print_paths =False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01f03",
   "metadata": {},
   "source": [
    "# Merge & save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_taxonomy(classif: str) -> dict:\n",
    "    cols = {\"Domain\": None, \"Phylum\": None, \"Class\": None,\n",
    "            \"Order\": None, \"Family\": None, \"Genus\": None, \"Species\": None}\n",
    "    if not isinstance(classif, str):\n",
    "        return cols\n",
    "    for token in classif.split(\";\"):\n",
    "        if \"__\" not in token:\n",
    "            continue\n",
    "        prefix, name = token.split(\"__\", 1)\n",
    "        if   prefix == \"d\": cols[\"Domain\"]  = name\n",
    "        elif prefix == \"p\": cols[\"Phylum\"]  = name\n",
    "        elif prefix == \"c\": cols[\"Class\"]   = name\n",
    "        elif prefix == \"o\": cols[\"Order\"]   = name\n",
    "        elif prefix == \"f\": cols[\"Family\"]  = name\n",
    "        elif prefix == \"g\": cols[\"Genus\"]   = name\n",
    "        elif prefix == \"s\": cols[\"Species\"] = name\n",
    "    return cols\n",
    "\n",
    "def prepare_mag_table(df_gtdb: pd.DataFrame, df_cov: pd.DataFrame, df_bin: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "Builds the final MAG table as required.\n",
    "Expected inputs:\n",
    "- df_gtdb: columns at least ['user_genome','classification','closest_genome_reference','closest_genome_ani']\n",
    "- df_cov: columns at least ['rname','endpos','numreads'] (+ optional 'sample_id')\n",
    "- df_bin: columns at least ['contig','bin'] + (from DASTool_summary.tsv) 'bin_score'\n",
    "Returns a DataFrame with columns:\n",
    "['mag_id','genome_size','bin_score','relative_abundance',\n",
    "'Domain','Phylum','Class','Order','Family','Genus','Species', 'closest_reference_genome_id','closest_reference_genome_ani']\n",
    "and prints how many records were rejected due to missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) map contig->bin and connect to coverage\n",
    "    # sanity dtype\n",
    "    cov = df_cov.copy()\n",
    "    cov.columns = [str(c).lstrip(\"#\") for c in cov.columns]\n",
    "    cov[\"endpos\"]   = pd.to_numeric(cov[\"endpos\"], errors=\"coerce\")\n",
    "    cov[\"numreads\"] = pd.to_numeric(cov[\"numreads\"], errors=\"coerce\")\n",
    "\n",
    "    contig2bin = df_bin[[\"contig\", \"bin\"]].dropna().copy()\n",
    "\n",
    "    cov_bin = cov.merge(contig2bin, left_on=\"rname\", right_on=\"contig\", how=\"inner\")\n",
    "\n",
    "# 2) genome_size: sum of contig lengths in the bin\n",
    "# I take the contig length as endpos (coverage counted from 1 to endpos)\n",
    "    contig_len = (cov_bin\n",
    "                  .groupby([\"bin\", \"rname\"], as_index=False)[\"endpos\"]\n",
    "                  .max())  # na wypadek duplikatów rname w pliku\n",
    "    genome_size = (contig_len\n",
    "                   .groupby(\"bin\", as_index=False)[\"endpos\"]\n",
    "                   .sum()\n",
    "                   .rename(columns={\"bin\": \"mag_id\", \"endpos\": \"genome_size\"}))\n",
    "\n",
    "    # 3) relative abundance: share of readings per bin\n",
    "    if \"sample_id\" in cov_bin.columns:\n",
    "        reads_per = (cov_bin.groupby([\"sample_id\", \"bin\"], as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = (cov_bin.groupby(\"sample_id\", as_index=False)[\"numreads\"]\n",
    "                       .sum()\n",
    "                       .rename(columns={\"numreads\": \"reads_total\"}))\n",
    "        rel = reads_per.merge(total_reads, on=\"sample_id\", how=\"left\")\n",
    "        rel[\"relative_abundance\"] = rel[\"reads_in_bin\"] / rel[\"reads_total\"]\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "# If I have multiple samples, duplicate mag_ids from different samples may result.\n",
    "# Consolidate by sum (or average). By default, I'll take the sum of the contributions (typically 1 sample => no influence).\n",
    "        rel = rel.groupby(\"mag_id\", as_index=False)[\"relative_abundance\"].sum()\n",
    "    else:\n",
    "        reads_per = (cov_bin.groupby(\"bin\", as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = reads_per[\"reads_in_bin\"].sum()\n",
    "        rel = reads_per.assign(relative_abundance=reads_per[\"reads_in_bin\"] / total_reads)\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "\n",
    "    # 4) bin_score from DASTool_summary\n",
    "    # Take unique bin_score per bin (sometimes repeated per contig).\n",
    "    if \"bin_score\" in df_bin.columns:\n",
    "        bs = (df_bin[[\"bin\", \"bin_score\"]]\n",
    "              .dropna(subset=[\"bin\"])\n",
    "              .drop_duplicates(subset=[\"bin\"]))\n",
    "        bs[\"bin_score\"] = pd.to_numeric(bs[\"bin_score\"], errors=\"coerce\")\n",
    "        bs = bs.rename(columns={\"bin\": \"mag_id\"})\n",
    "    else:\n",
    "        # if no column in input\n",
    "        bs = pd.DataFrame(columns=[\"mag_id\", \"bin_score\"])\n",
    "\n",
    "    # 5) GTDB: taxonomy + closest genome\n",
    "    gtdb = df_gtdb[[\"user_genome\", \"classification\",\n",
    "                    \"closest_genome_reference\", \"closest_genome_ani\"]].copy()\n",
    "\n",
    "    tax = gtdb[\"classification\"].apply(_split_taxonomy).apply(pd.Series)\n",
    "    gtdb_clean = pd.concat([gtdb.drop(columns=[\"classification\"]), tax], axis=1)\n",
    "    gtdb_clean = gtdb_clean.rename(columns={\n",
    "        \"user_genome\": \"mag_id\",\n",
    "        \"closest_genome_reference\": \"closest_reference_genome_id\",\n",
    "        \"closest_genome_ani\": \"closest_reference_genome_ani\"\n",
    "    })\n",
    "    gtdb_clean[\"closest_reference_genome_ani\"] = pd.to_numeric(\n",
    "        gtdb_clean[\"closest_reference_genome_ani\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 6) Merging everything by mag_id\n",
    "    merged = (genome_size\n",
    "              .merge(rel, on=\"mag_id\", how=\"left\")\n",
    "              .merge(bs, on=\"mag_id\",  how=\"left\")\n",
    "              .merge(gtdb_clean, on=\"mag_id\", how=\"left\"))\n",
    "\n",
    "    # 7) First select only the required columns\n",
    "    wanted = [\n",
    "        \"mag_id\",\n",
    "        \"genome_size\",\n",
    "        \"bin_score\",\n",
    "        \"relative_abundance\",\n",
    "        \"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\",\n",
    "        \"closest_reference_genome_id\",\n",
    "        \"closest_reference_genome_ani\",\n",
    "    ]\n",
    "    out = merged[wanted].copy()\n",
    "\n",
    "    # Now remove missing records and report\n",
    "    before = len(out)\n",
    "    out_clean = out.dropna()\n",
    "    removed = before - len(out_clean)\n",
    "    logger.info(f\"Usunięto {removed} z {before} rekordów z brakami (NaN/NULL).\")\n",
    "\n",
    "    return out_clean\n",
    "\n",
    "df_mag = prepare_mag_table(df_gtdb, df_cov, df_bin)\n",
    "\n",
    "# Preview of the first few lines\n",
    "print(df_mag.head())\n",
    "\n",
    "# Writing to a file\n",
    "df_mag.to_csv(\"MAG_table.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ed894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
