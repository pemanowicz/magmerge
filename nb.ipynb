{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb03876",
   "metadata": {},
   "source": [
    "# Load & execute pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "from src.magmerge.pipelins import pipeline_Binning, pipeline_COVERAGE, pipeline_GTDBTK\n",
    "\n",
    "\n",
    "# Execute pipelines\n",
    "df_bin = pipeline_Binning(\"python_paths.csv\", print_paths =False)\n",
    "df_cov = pipeline_COVERAGE(\"python_paths.csv\", print_paths =False)\n",
    "df_gtdb = pipeline_GTDBTK(\"python_paths.csv\", print_paths =False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01f03",
   "metadata": {},
   "source": [
    "# Merge & save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_taxonomy(classif: str) -> dict:\n",
    "    cols = {\"Domain\": None, \"Phylum\": None, \"Class\": None,\n",
    "            \"Order\": None, \"Family\": None, \"Genus\": None, \"Species\": None}\n",
    "    if not isinstance(classif, str):\n",
    "        return cols\n",
    "    for token in classif.split(\";\"):\n",
    "        if \"__\" not in token:\n",
    "            continue\n",
    "        prefix, name = token.split(\"__\", 1)\n",
    "        if   prefix == \"d\": cols[\"Domain\"]  = name\n",
    "        elif prefix == \"p\": cols[\"Phylum\"]  = name\n",
    "        elif prefix == \"c\": cols[\"Class\"]   = name\n",
    "        elif prefix == \"o\": cols[\"Order\"]   = name\n",
    "        elif prefix == \"f\": cols[\"Family\"]  = name\n",
    "        elif prefix == \"g\": cols[\"Genus\"]   = name\n",
    "        elif prefix == \"s\": cols[\"Species\"] = name\n",
    "    return cols\n",
    "\n",
    "def prepare_mag_table(df_gtdb: pd.DataFrame, df_cov: pd.DataFrame, df_bin: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "Builds the final MAG table as required.\n",
    "Expected inputs:\n",
    "- df_gtdb: columns at least ['user_genome','classification','closest_genome_reference','closest_genome_ani']\n",
    "- df_cov: columns at least ['rname','endpos','numreads'] (+ optional 'sample_id')\n",
    "- df_bin: columns at least ['contig','bin'] + (from DASTool_summary.tsv) 'bin_score'\n",
    "Returns a DataFrame with columns:\n",
    "['mag_id','genome_size','bin_score','relative_abundance',\n",
    "'Domain','Phylum','Class','Order','Family','Genus','Species', 'closest_reference_genome_id','closest_reference_genome_ani']\n",
    "and prints how many records were rejected due to missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) map contig->bin and connect to coverage\n",
    "    # sanity dtype\n",
    "    cov = df_cov.copy()\n",
    "    cov.columns = [str(c).lstrip(\"#\") for c in cov.columns]\n",
    "    cov[\"endpos\"]   = pd.to_numeric(cov[\"endpos\"], errors=\"coerce\")\n",
    "    cov[\"numreads\"] = pd.to_numeric(cov[\"numreads\"], errors=\"coerce\")\n",
    "\n",
    "    contig2bin = df_bin[[\"contig\", \"bin\"]].dropna().copy()\n",
    "\n",
    "    cov_bin = cov.merge(contig2bin, left_on=\"rname\", right_on=\"contig\", how=\"inner\")\n",
    "\n",
    "# 2) genome_size: sum of contig lengths in the bin\n",
    "# I take the contig length as endpos (coverage counted from 1 to endpos)\n",
    "    contig_len = (cov_bin\n",
    "                  .groupby([\"bin\", \"rname\"], as_index=False)[\"endpos\"]\n",
    "                  .max())  # na wypadek duplikatów rname w pliku\n",
    "    genome_size = (contig_len\n",
    "                   .groupby(\"bin\", as_index=False)[\"endpos\"]\n",
    "                   .sum()\n",
    "                   .rename(columns={\"bin\": \"mag_id\", \"endpos\": \"genome_size\"}))\n",
    "\n",
    "    # 3) relative abundance: share of readings per bin\n",
    "    if \"sample_id\" in cov_bin.columns:\n",
    "        reads_per = (cov_bin.groupby([\"sample_id\", \"bin\"], as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = (cov_bin.groupby(\"sample_id\", as_index=False)[\"numreads\"]\n",
    "                       .sum()\n",
    "                       .rename(columns={\"numreads\": \"reads_total\"}))\n",
    "        rel = reads_per.merge(total_reads, on=\"sample_id\", how=\"left\")\n",
    "        rel[\"relative_abundance\"] = rel[\"reads_in_bin\"] / rel[\"reads_total\"]\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "# If I have multiple samples, duplicate mag_ids from different samples may result.\n",
    "# Consolidate by sum (or average). By default, I'll take the sum of the contributions (typically 1 sample => no influence).\n",
    "        rel = rel.groupby(\"mag_id\", as_index=False)[\"relative_abundance\"].sum()\n",
    "    else:\n",
    "        reads_per = (cov_bin.groupby(\"bin\", as_index=False)[\"numreads\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"numreads\": \"reads_in_bin\"}))\n",
    "        total_reads = reads_per[\"reads_in_bin\"].sum()\n",
    "        rel = reads_per.assign(relative_abundance=reads_per[\"reads_in_bin\"] / total_reads)\n",
    "        rel = rel.rename(columns={\"bin\": \"mag_id\"})[[\"mag_id\", \"relative_abundance\"]]\n",
    "\n",
    "    # 4) bin_score from DASTool_summary\n",
    "    # Take unique bin_score per bin (sometimes repeated per contig).\n",
    "    if \"bin_score\" in df_bin.columns:\n",
    "        bs = (df_bin[[\"bin\", \"bin_score\"]]\n",
    "              .dropna(subset=[\"bin\"])\n",
    "              .drop_duplicates(subset=[\"bin\"]))\n",
    "        bs[\"bin_score\"] = pd.to_numeric(bs[\"bin_score\"], errors=\"coerce\")\n",
    "        bs = bs.rename(columns={\"bin\": \"mag_id\"})\n",
    "    else:\n",
    "        # if no column in input\n",
    "        bs = pd.DataFrame(columns=[\"mag_id\", \"bin_score\"])\n",
    "\n",
    "    # 5) GTDB: taxonomy + closest genome\n",
    "    gtdb = df_gtdb[[\"user_genome\", \"classification\",\n",
    "                    \"closest_genome_reference\", \"closest_genome_ani\"]].copy()\n",
    "\n",
    "    tax = gtdb[\"classification\"].apply(_split_taxonomy).apply(pd.Series)\n",
    "    gtdb_clean = pd.concat([gtdb.drop(columns=[\"classification\"]), tax], axis=1)\n",
    "    gtdb_clean = gtdb_clean.rename(columns={\n",
    "        \"user_genome\": \"mag_id\",\n",
    "        \"closest_genome_reference\": \"closest_reference_genome_id\",\n",
    "        \"closest_genome_ani\": \"closest_reference_genome_ani\"\n",
    "    })\n",
    "    gtdb_clean[\"closest_reference_genome_ani\"] = pd.to_numeric(\n",
    "        gtdb_clean[\"closest_reference_genome_ani\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 6) Merging everything by mag_id\n",
    "    merged = (genome_size\n",
    "              .merge(rel, on=\"mag_id\", how=\"left\")\n",
    "              .merge(bs, on=\"mag_id\",  how=\"left\")\n",
    "              .merge(gtdb_clean, on=\"mag_id\", how=\"left\"))\n",
    "\n",
    "    # 7) First select only the required columns\n",
    "    wanted = [\n",
    "        \"mag_id\",\n",
    "        \"genome_size\",\n",
    "        \"bin_score\",\n",
    "        \"relative_abundance\",\n",
    "        \"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\",\n",
    "        \"closest_reference_genome_id\",\n",
    "        \"closest_reference_genome_ani\",\n",
    "    ]\n",
    "    out = merged[wanted].copy()\n",
    "\n",
    "    # Now remove missing records and report\n",
    "    before = len(out)\n",
    "    out_clean = out.dropna()\n",
    "    removed = before - len(out_clean)\n",
    "    logger.info(f\"Usunięto {removed} z {before} rekordów z brakami (NaN/NULL).\")\n",
    "\n",
    "    return out_clean\n",
    "\n",
    "df_mag = prepare_mag_table(df_gtdb, df_cov, df_bin)\n",
    "\n",
    "# Preview of the first few lines\n",
    "print(df_mag.head())\n",
    "\n",
    "# Writing to a file\n",
    "df_mag.to_csv(\"MAG_table.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
